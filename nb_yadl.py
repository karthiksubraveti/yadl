
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/yadl.ipynb


from torch import nn
class LinearModel():
    def __init__(self, num_in, num_out):
        self.inp = None
        self.out = None
        self.wt = torch.randn([num_in, num_out]) / math.sqrt(num_in)
        self.b = torch.zeros([num_out])

    def __call__(self, inp):
        self.inp = inp
        self.out =  inp @ self.wt + self.b
        return self.out

    def backward(self, ):
        self.wt.grad = self.inp.t() @ self.out.grad
        self.inp.grad = self.out.grad @ self.wt.t()
        self.b.grad = self.out.grad.sum(0)

class Relu():
    def __init__(self,):
        self.inp = None
        self.out = None

    def __call__(self, inp):
        self.inp = inp
        self.out = inp.clamp_(0,)
        return self.out

    def backward(self, ):
        self.inp.grad = ( self.inp > 0 ).float() * self.out.grad

class Mse():
    def __init__(self, ):
        self.inp = None
        self.out = None

    def __call__(self, output, target):
        self.inp = output
        self.out = target
        return((output.squeeze() - target).pow(2)).mean()

    def backward(self,):
        self.inp.grad = ((self.inp.squeeze() - self.out) / self.inp.shape[0]).unsqueeze(-1)

class Sequential():
    def __init__(self, lr=0.01, layers=[],  loss_fn=None,):
        self.models = []
        self.loss_fn = loss_fn
        if not loss_fn:
            self.loss_fn = Mse()

        self.lr = lr
        for i in range(len(layers) - 1):
            self.models.append(LinearModel(layers[i], layers[i + 1]))
            self.models.append(Relu())
        self.models.append(LinearModel(layers[-1], 1))

    def __call__(self, inp, targ):
        x = inp
        for model in self.models:
            x = model(x)
        return self.loss_fn(x, targ)

    def backward(self, ):
        self.loss_fn.backward()
        for model in reversed(self.models):
            model.backward()

    def update(self, ):
        for model in self.models:
            if hasattr(model, "wt"):
                model.wt = model.wt - (self.lr * model.wt.grad)
